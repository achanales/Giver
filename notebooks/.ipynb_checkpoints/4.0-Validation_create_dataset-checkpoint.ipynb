{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pdb\n",
    "import importlib\n",
    "import os\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keywords_functions\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keywords_functions' from '/Users/avichanales/Dropbox/Insight/Project/insight_project/notebooks/keywords_functions.py'>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(keywords_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom functions\n",
    "from nlp_functions import preprocess_spacy\n",
    "from keywords_functions import get_key, find_keywords,find_keywords_df, tfidf_article,preprocess_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir =  os.path.abspath(os.path.join(os.getcwd() ,\"../../\"))\n",
    "file_name = os.path.join(root_dir,'raw_data','news_data_clean.csv')\n",
    "all_news = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir =  os.path.abspath(os.path.join(os.getcwd() ,\"../\"))\n",
    "file_name = (os.path.join(project_dir,'data','processed','charity_data_clean.csv')\n",
    "all_charity = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude charities with words as names or popular acronyms\n",
    "\n",
    "exclude ={\"FIRST\", \"ECHO\", \"ICA\", \"JOIN\", 'Lumberyard', 'Leaven', 'LEAP', \n",
    "          'Mouse', 'The Mount','MATTER','making change','NPR','Neighbors',\n",
    "          'NAF','Our Kids','Puente','Public Knowledge','Provide','ProPublica','PRISM', \n",
    "          'Polaris','Pillars','PATH','PAI','The Rose', 'Roca','Rare','RAFT', 'Step Up',\n",
    "          'STEP','STARS','SPUR','SOME','SMART','SHARE', 'Second Chance','SBP','SAGE',\n",
    "          'Trail Blazers','USO','Unbound','Ventures','World Vision','The Ark','Amara',\n",
    "          'Alternatives','ACCESS','Breakthrough','Bottom Line','Commonweal','Commentary',\n",
    "          'CLASP',\"CET\",'Ceres','Centro','Center Stage','CARE','DREAM','Demos','Equest',\n",
    "          'Endeavor','Fulfill','FIRST','HERE'}\n",
    "\n",
    "\n",
    "all_charity_use = all_charity[~all_charity.name.isin(exclude)]\n",
    "\n",
    "# Get charity names\n",
    "charity_names= all_charity_use['name'].astype('str')\n",
    "\n",
    "# Trim news database\n",
    "all_news_use = all_news[['title','publication','content']]\n",
    "\n",
    "# Drop rows that dont have a title, publicaition, and article text\n",
    "all_news_use = all_news_use.dropna()\n",
    "\n",
    "# Remove headlines with fewer than 4 words\n",
    "all_news_use['headline_length'] = all_news_use.apply(lambda x: len(x['title'].split(\" \")),axis=1)\n",
    "all_news_use = all_news_use[all_news_use['headline_length'] >3]\n",
    "\n",
    "# Remove publication name from title\n",
    "all_news_use['title'] = all_news_use.apply(lambda x: x['title'].replace(x['publication'],\"\"),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Search for Articles that mention a charity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find database of articles that mention any of the charities in the database\n",
    "\n",
    "def df_topic_articles(df, search_term):\n",
    "    'Return dataframe of articles that conatin search term '\n",
    "    \n",
    "    # Create column \"topic\" that returns the number of times a search time was mentioned in an articles or -1 if the search term wasnt found\n",
    "    df['topic'] = df['content'].str.find(search_term)\n",
    "    \n",
    "    #Filter out articles with no search term\n",
    "    df_sub = df[df['topic']>-1]\n",
    "    \n",
    "    #If the dataset isnt empty for this search term\n",
    "    if df_sub.empty == False:\n",
    "        \n",
    "        df_sub = df_sub[['title','content']]\n",
    "        \n",
    "        #Add charity column \n",
    "        df_sub['charity'] = search_term\n",
    "        \n",
    "        #Drop duplicate entries and missing titles\n",
    "        df_sub = df_sub[(~df_sub.duplicated())]\n",
    "        df_sub = df_sub[df_sub['title'] != ' ']\n",
    "    \n",
    "        return df_sub\n",
    "\n",
    "\n",
    "article_list = []\n",
    "for name in charity_names:\n",
    "    article_list.append(df_topic_articles(all_news_use,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = pd.concat(article_list)\n",
    "article_list = article_list.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Find top 10 keywords for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word frequency model from news corpus data\n",
    "\n",
    "with open(os.path.join(project_dir,'models','news','news_tfidf_min_300_max_0.2.pickle, 'rb') as handle:\n",
    "    news_tfidf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = article_list.apply(find_keywords_df,news_tfidf=news_tfidf,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list.to_csv(os.path.join(project_dir,'data','processed','validation_set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
